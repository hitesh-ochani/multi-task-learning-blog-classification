{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import load_data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from models.RCNN_MTL2 import RCNN_MTL\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(999)\n",
    "import random\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 140361\n",
      "Vector size of Text Vocabulary:  torch.Size([140361, 100])\n",
      "Label Length: 2\n",
      "AGE BINS Label Length: 5\n"
     ]
    }
   ],
   "source": [
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data.load_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "\n",
    "def train_model(model, train_iter, epoch):\n",
    "\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "#     model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text\n",
    "        target = batch.label\n",
    "        target2 = batch.age_bins\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        target2 = torch.autograd.Variable(target2).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "            target2 = target2.cuda()\n",
    "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "        prediction_gender = prediction[0]\n",
    "        prediction_age = prediction[1]\n",
    "        loss1 = loss_fn(prediction_gender, target)\n",
    "        loss2 = loss_fn(prediction_age, target2)\n",
    "        loss = loss1 + loss2\n",
    "        #*******To change below stuff *******\n",
    "        num_corrects = (torch.max(prediction_gender, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target2 = batch.age_bins\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            target2 = torch.autograd.Variable(target2).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "                target2 = target2.cuda()\n",
    "            prediction = model(text)\n",
    "            prediction_gender = prediction[0]\n",
    "            prediction_age = prediction[1]\n",
    "            loss1 = loss_fn(prediction_gender, target)\n",
    "            loss2 = loss_fn(prediction_age, target2)\n",
    "            loss = loss1 + loss2\n",
    "            num_corrects = (torch.max(prediction_gender, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshochani/anaconda3/envs/statnlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "output_size2 = 9\n",
    "hidden_size = 256\n",
    "embedding_length = 100\n",
    "\n",
    "model = RCNN_MTL(batch_size, output_size, output_size2, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshochani/.local/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.807, Train Acc: 57.50%, Val. Loss: 1.649092, Val. Acc: 56.72%\n",
      "Epoch: 02, Train Loss: 1.608, Train Acc: 63.21%, Val. Loss: 1.555423, Val. Acc: 62.75%\n",
      "Epoch: 03, Train Loss: 1.512, Train Acc: 69.58%, Val. Loss: 1.621416, Val. Acc: 63.72%\n",
      "Epoch: 04, Train Loss: 1.405, Train Acc: 72.38%, Val. Loss: 1.520639, Val. Acc: 64.16%\n",
      "Epoch: 05, Train Loss: 1.270, Train Acc: 77.12%, Val. Loss: 1.538335, Val. Acc: 63.78%\n",
      "Epoch: 06, Train Loss: 1.137, Train Acc: 81.50%, Val. Loss: 1.538000, Val. Acc: 63.38%\n",
      "Epoch: 07, Train Loss: 1.002, Train Acc: 84.50%, Val. Loss: 1.601177, Val. Acc: 64.53%\n",
      "Epoch: 08, Train Loss: 0.823, Train Acc: 89.88%, Val. Loss: 1.631925, Val. Acc: 64.19%\n",
      "Epoch: 09, Train Loss: 0.638, Train Acc: 94.17%, Val. Loss: 1.643790, Val. Acc: 64.94%\n",
      "Epoch: 10, Train Loss: 0.448, Train Acc: 96.46%, Val. Loss: 1.798308, Val. Acc: 64.88%\n",
      "Epoch: 11, Train Loss: 0.300, Train Acc: 98.04%, Val. Loss: 1.933271, Val. Acc: 64.47%\n",
      "Epoch: 12, Train Loss: 0.183, Train Acc: 98.04%, Val. Loss: 2.165072, Val. Acc: 64.84%\n",
      "Epoch: 13, Train Loss: 0.110, Train Acc: 98.29%, Val. Loss: 2.238578, Val. Acc: 63.88%\n",
      "Epoch: 14, Train Loss: 0.077, Train Acc: 98.38%, Val. Loss: 2.424298, Val. Acc: 64.56%\n",
      "Epoch: 15, Train Loss: 0.062, Train Acc: 98.38%, Val. Loss: 2.739031, Val. Acc: 63.94%\n",
      "Epoch: 16, Train Loss: 0.047, Train Acc: 98.42%, Val. Loss: 2.580202, Val. Acc: 63.19%\n",
      "Epoch: 17, Train Loss: 0.031, Train Acc: 98.58%, Val. Loss: 2.891710, Val. Acc: 64.88%\n",
      "Epoch: 18, Train Loss: 0.046, Train Acc: 98.29%, Val. Loss: 2.800621, Val. Acc: 63.84%\n",
      "Epoch: 19, Train Loss: 0.026, Train Acc: 98.58%, Val. Loss: 3.026337, Val. Acc: 65.28%\n",
      "Epoch: 20, Train Loss: 0.025, Train Acc: 98.62%, Val. Loss: 2.985962, Val. Acc: 62.84%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "\n",
    "    pickle.dump(model, open(\"model_rcnn_idx\"+str(epoch)+\".pickle\",\"wb\"))\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshochani/.local/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.841, Test Acc: 57.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshochani/anaconda3/envs/statnlp/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7a73c24f95b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_sen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "test_sen2 = TEXT.preprocess(test_sen2)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
