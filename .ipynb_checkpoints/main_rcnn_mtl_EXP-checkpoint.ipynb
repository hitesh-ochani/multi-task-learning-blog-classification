{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import load_data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from models.RCNN_MTL2 import RCNN_MTL\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(999)\n",
    "import random\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 382804\n",
      "Vector size of Text Vocabulary:  torch.Size([382804, 100])\n",
      "Label Length: 2\n",
      "AGE BINS Label Length: 5\n"
     ]
    }
   ],
   "source": [
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data.load_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "\n",
    "def train_model(model, train_iter, epoch):\n",
    "\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "#     model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text\n",
    "        target = batch.label\n",
    "        target2 = batch.age_bins\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        target2 = torch.autograd.Variable(target2).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "            target2 = target2.cuda()\n",
    "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "        prediction_gender = prediction[0]\n",
    "        prediction_age = prediction[1]\n",
    "        loss1 = loss_fn(prediction_gender, target)\n",
    "        loss2 = loss_fn(prediction_age, target2)\n",
    "        loss = loss1 + loss2\n",
    "        #*******To change below stuff *******\n",
    "        num_corrects = (torch.max(prediction_gender, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target2 = batch.age_bins\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            target2 = torch.autograd.Variable(target2).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "                target2 = target2.cuda()\n",
    "            prediction = model(text)\n",
    "            prediction_gender = prediction[0]\n",
    "            prediction_age = prediction[1]\n",
    "            loss1 = loss_fn(prediction_gender, target)\n",
    "            loss2 = loss_fn(prediction_age, target2)\n",
    "            loss = loss1 + loss2\n",
    "            num_corrects = (torch.max(prediction_gender, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshochani/anaconda3/envs/statnlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "output_size2 = 9\n",
    "hidden_size = 256\n",
    "embedding_length = 100\n",
    "\n",
    "model = RCNN_MTL(batch_size, output_size, output_size2, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 100, Training Loss: 2.0854, Training Accuracy:  53.12%\n",
      "Epoch: 1, Idx: 200, Training Loss: 1.7696, Training Accuracy:  68.75%\n",
      "Epoch: 1, Idx: 300, Training Loss: 1.2828, Training Accuracy:  65.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshochani/.local/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.653, Train Acc: 62.68%, Val. Loss: 1.570326, Val. Acc: 65.21%\n",
      "Epoch: 2, Idx: 100, Training Loss: 1.7296, Training Accuracy:  53.12%\n",
      "Epoch: 2, Idx: 200, Training Loss: 1.2015, Training Accuracy:  68.75%\n",
      "Epoch: 2, Idx: 300, Training Loss: 1.4796, Training Accuracy:  68.75%\n",
      "Epoch: 02, Train Loss: 1.486, Train Acc: 69.02%, Val. Loss: 1.545994, Val. Acc: 66.17%\n",
      "Epoch: 3, Idx: 100, Training Loss: 1.1808, Training Accuracy:  71.88%\n",
      "Epoch: 3, Idx: 200, Training Loss: 1.6188, Training Accuracy:  68.75%\n",
      "Epoch: 3, Idx: 300, Training Loss: 1.4098, Training Accuracy:  71.88%\n",
      "Epoch: 03, Train Loss: 1.390, Train Acc: 71.69%, Val. Loss: 1.502102, Val. Acc: 69.39%\n",
      "Epoch: 4, Idx: 100, Training Loss: 1.2573, Training Accuracy:  78.12%\n",
      "Epoch: 4, Idx: 200, Training Loss: 1.4353, Training Accuracy:  78.12%\n",
      "Epoch: 4, Idx: 300, Training Loss: 1.3358, Training Accuracy:  75.00%\n",
      "Epoch: 04, Train Loss: 1.283, Train Acc: 74.89%, Val. Loss: 1.523204, Val. Acc: 68.64%\n",
      "Epoch: 5, Idx: 100, Training Loss: 1.0906, Training Accuracy:  71.88%\n",
      "Epoch: 5, Idx: 200, Training Loss: 0.8194, Training Accuracy:  81.25%\n",
      "Epoch: 5, Idx: 300, Training Loss: 1.3461, Training Accuracy:  65.62%\n",
      "Epoch: 05, Train Loss: 1.141, Train Acc: 79.03%, Val. Loss: 1.590979, Val. Acc: 68.61%\n",
      "Epoch: 6, Idx: 100, Training Loss: 1.0812, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 200, Training Loss: 0.7018, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 300, Training Loss: 0.7094, Training Accuracy:  87.50%\n",
      "Epoch: 06, Train Loss: 0.957, Train Acc: 84.41%, Val. Loss: 1.665734, Val. Acc: 67.19%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.3724, Training Accuracy:  100.00%\n",
      "Epoch: 7, Idx: 200, Training Loss: 0.7273, Training Accuracy:  90.62%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "\n",
    "    pickle.dump(model, open(\"model_rcnn_idx\"+str(epoch)+\".pickle\",\"wb\"))\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "test_sen2 = TEXT.preprocess(test_sen2)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
